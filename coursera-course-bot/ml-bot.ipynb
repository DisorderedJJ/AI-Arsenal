{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6005bec-8382-4c64-ac53-0e0e3f2b4ba5",
   "metadata": {},
   "source": [
    "# Coursera AI Engineering Course Assistant Bot\n",
    "\n",
    "This is a bot for answer the questions which user asks regarding the machine learning course that was done in coursera. This is implemnted using the learned concepts of RAG and lang chain thoughout the course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7788f4-d8e2-457a-a2d2-7894b7de599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Libraries\n",
    "!pip install torch\n",
    "!pip install transformers langchain_community langchain_text_splitters langchain_core\n",
    "!pip install sentence-transformers\n",
    "!pip install chromadb\n",
    "!pip install huggingface_hub\n",
    "!pip install accelerate\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaedb10-3ead-4cd2-9634-fbf2c352aba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13463829-c2bd-48bc-a3b7-18fbceca1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module Imports\n",
    "import os\n",
    "import torch\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e0db9-6cbd-4cfa-a68c-2a2a4afe23f1",
   "metadata": {},
   "source": [
    "We need to define the folder that contain our knowledge base, which are the documents of the transcripted vedios in coursera course vedios\n",
    "We also need to define the prompt template that we are going to use for the langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f3d916-de37-4871-93d0-edffe84935eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path: str = \"data/\"\n",
    "\n",
    "template = \"\"\"You are an AI assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer the question concisely.\n",
    "If you don't know the answer, just say that you don't know.\n",
    "Keep the answer to three sentences maximum.\n",
    "\n",
    "Question: {question}\n",
    "Context: {context}\n",
    "Answer:\"\"\"\n",
    "\n",
    "chain_save_path = \"rag_chain.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19606409-87d6-4c3a-ad7b-701a95c546c2",
   "metadata": {},
   "source": [
    "# Preprocess Stage\n",
    "In here we are going to implement functions for the preporcess steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96dbcd-0412-4b64-993d-558f2cd3b346",
   "metadata": {},
   "source": [
    "### Preprocess the document files we are going to use as our data bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1fad4-2cdb-4659-8092-ec0be1f6a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_files():\n",
    "    for filename in os.listdir(folder_path):\n",
    "        full_old_path = os.path.join(folder_path, filename)\n",
    "        new_filename = filename.replace(\",\",\"\").replace(\" \", \"-\")\n",
    "        full_new_path = os.path.join(folder_path, new_filename)\n",
    "        os.rename(full_old_path, full_new_path)\n",
    "\n",
    "    txt_list: list[str] = [filename for filename in os.listdir(folder_path) if os.path.splitext(filename)[1] =='.txt']\n",
    "    pdf_list: list[str] = [filename for filename in os.listdir(folder_path) if os.path.splitext(filename)[1] =='.pdf']\n",
    "    return txt_list, pdf_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0cb9fb-1518-4fa7-b48b-a9631aafd487",
   "metadata": {},
   "source": [
    "### Function to load txt and pdf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e12b1-117c-4832-8d2b-1d8679776eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TXT loader\n",
    "def text_loader(txt_filepath: str):\n",
    "    txt_loader = TextLoader(txt_filepath)\n",
    "    return txt_loader.load()\n",
    "\n",
    "# PDF loader\n",
    "def pdf_file_loader(pdf_filepath: str):\n",
    "    pdf_loader = PyPDFLoader(pdf_filepath, extract_images=False,)\n",
    "    return pdf_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95710653-ab6d-4673-9a6e-6372c0a04717",
   "metadata": {},
   "source": [
    "### In here we define the text splitter to split the data of the files to chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed5f77c-c050-4f20-a1f3-960eca0e09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_for_chunks(document):\n",
    "    text_splitter= RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    return text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e19be8-d3dd-4bb2-b12d-daa1be6fa480",
   "metadata": {},
   "source": [
    "# Model Creating Stage\n",
    "In here we define the function to load a llm model and create the pipeline to use the llm for generation part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b1a9bf-4099-4393-a8ef-b9bce29f96ab",
   "metadata": {},
   "source": [
    "### Create LLM for langchain for the text generation part using the retrieved documents and the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e7e93-bd50-442f-8cfb-3c0dd4dbb127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_LLM():\n",
    "    model_name: str = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map='auto'\n",
    "    )\n",
    "    pipe = pipeline(\n",
    "        'text-generation',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1bfad0-1e9c-45ca-9efb-019c99532cfd",
   "metadata": {},
   "source": [
    "# VectorDB Creating Stage\n",
    "In here we define the function to create the vectorDB. we use vector DB to store the embbeding context of the knowledge base "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4b8209-47dc-49e9-9eb8-96d2c1e8d6ec",
   "metadata": {},
   "source": [
    "### Create Embedding model to generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7df136-1187-4ac0-9628-8d1a7458cc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embedding_model():\n",
    "    embedding_model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "    embeddings_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        model_kwargs={'device': 'cpu'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "    )\n",
    "    return embeddings_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d87c9-9f4c-4b1b-8189-7f1696261c58",
   "metadata": {},
   "source": [
    "### Create Vector DB for save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f993e-b9bf-4b87-ab34-f768abc0461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_vector_DB(embeddings_model):\n",
    "    vector_db_path = \"./chroma_db_hf_embeddings\"\n",
    "    if os.path.exists(vector_db_path) and os.listdir(vector_db_path):\n",
    "        vector_db = Chroma(\n",
    "            persist_directory=vector_db_path,\n",
    "            embedding_function=embeddings_model\n",
    "        )\n",
    "    else:\n",
    "        vector_db=Chroma.from_documents(\n",
    "            documents=[],\n",
    "            embedding=embeddings_model,\n",
    "            persist_directory=vector_db_path\n",
    "        )\n",
    "        vector_db.persist()\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50792677-c3c3-4de0-9e05-56bbaae062ad",
   "metadata": {},
   "source": [
    "### Load document to vectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604f3e9-0e00-427d-ab34-a5e1e047144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_to_vector_db(doc_chunks, vector_db):\n",
    "    vector_db.add_documents(doc_chunks)\n",
    "    vector_db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516b11b-74cd-4202-a935-41846208b4f7",
   "metadata": {},
   "source": [
    "### Load all documents, split it to chunks and then save them in vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4512da3a-fc5d-48d9-b8e7-ddb269d458bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_and_load(txt_list: list[str], pdf_list: list[str], vector_db):\n",
    "    for txt_filename in txt_list:\n",
    "        full_path = os.path.join(folder_path, txt_filename)\n",
    "        loaded_file = text_loader(full_path)\n",
    "        chunks = split_text_for_chunks(loaded_file)\n",
    "        load_chunks_to_vector_db(chunks, vector_db)\n",
    "\n",
    "    for pdf_filename in pdf_list:\n",
    "        try:\n",
    "            full_path = os.path.join(folder_path, pdf_filename)\n",
    "            loaded_file = text_loader(full_path)\n",
    "            chunks = split_text_for_chunks(loaded_file)\n",
    "            load_chunks_to_vector_db(chunks, vector_db)\n",
    "        except Exception as e:\n",
    "            print(f'Exception occured when reading {pdf_filename}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d983dcf-b59b-4430-899e-4601437f1637",
   "metadata": {},
   "source": [
    "# LangChain Create Stage\n",
    "In here we are going to assemble all the functions that we created above and create the final langchain to get the user query and answer to that by gaining knowledge from our knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf153979-570d-489c-965c-aea9e02e7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langchain():\n",
    "    txt_list, pdf_list = preprocess_data_files()\n",
    "    embedding_model = initialize_embedding_model()\n",
    "    vector_db = intialize_vector_DB(embedding_model)\n",
    "    read_files_and_load(txt_list, pdf_list, vector_db)\n",
    "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "    llm = define_LLM()\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n",
    "    )\n",
    "    return rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571c296c-7807-4efc-9b2d-fcadde988230",
   "metadata": {},
   "source": [
    "# Execution of the QA Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c018a9-afe2-490c-a2f3-9f6f7fc01646",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_rag_chain = create_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc97ee2-fcf7-4e76-bf14-c05ff2de33be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(user_question):\n",
    "    print(user_question)\n",
    "    if not user_question:\n",
    "        return \"Please enter a query to get a response.\"\n",
    "    try:\n",
    "        response = qa_rag_chain.invoke(user_question)\n",
    "        answer_prefix = \"Answer:\"\n",
    "        answer_start_index = response.find(answer_prefix)\n",
    "        if answer_start_index != -1:\n",
    "            extracted_answer = response[answer_start_index + len(answer_prefix):].strip()\n",
    "        else:\n",
    "            print(\"Warning: 'Answer:' prefix not found in response. Printing full output.\")\n",
    "            extracted_answer = \"\"\n",
    "        return extracted_answer\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31192ad1-0380-4a89-8291-73171b0f82f3",
   "metadata": {},
   "source": [
    "# Create A User Interface to Interact with BOT\n",
    "\n",
    "Here we create a interactive user interface using dradio, which is a python library developed to implement UIs to AI applications specifically. Here we implement simple UI with one text box to get the user input and one output box to show the response to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a49f5c-8faf-4de1-8ff0-b83ac2b4f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=process_query, \n",
    "    inputs=gr.Textbox(\n",
    "        lines=5,\n",
    "        label=\"Your Input Query:\",\n",
    "        placeholder=\"Type your question here...\"\n",
    "    ),\n",
    "    outputs=gr.Textbox(\n",
    "        lines=10,\n",
    "        label=\"Output Response:\",\n",
    "        interactive=False\n",
    "    ),\n",
    "    title=\"Simple RAG Query Interface\",\n",
    "    description=\"Enter a query and get a response from the (simulated) RAG system.\"\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca330bba-c055-419b-bf5b-18126f7d6aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
